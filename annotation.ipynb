{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-20T16:52:36.338018Z","iopub.execute_input":"2023-06-20T16:52:36.338855Z","iopub.status.idle":"2023-06-20T16:52:36.360545Z","shell.execute_reply.started":"2023-06-20T16:52:36.338821Z","shell.execute_reply":"2023-06-20T16:52:36.359509Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/human-annotation/human_annotated_tweets.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom transformers import XLMRobertaTokenizer, TFXLMRobertaForSequenceClassification\nimport tensorflow as tf\n","metadata":{"execution":{"iopub.status.busy":"2023-06-20T16:52:36.362692Z","iopub.execute_input":"2023-06-20T16:52:36.363355Z","iopub.status.idle":"2023-06-20T16:52:51.997400Z","shell.execute_reply.started":"2023-06-20T16:52:36.363289Z","shell.execute_reply":"2023-06-20T16:52:51.996403Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/human-annotation/human_annotated_tweets.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-20T16:52:51.999016Z","iopub.execute_input":"2023-06-20T16:52:51.999847Z","iopub.status.idle":"2023-06-20T16:52:52.065153Z","shell.execute_reply.started":"2023-06-20T16:52:51.999812Z","shell.execute_reply":"2023-06-20T16:52:52.064178Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-06-20T16:52:52.067679Z","iopub.execute_input":"2023-06-20T16:52:52.068047Z","iopub.status.idle":"2023-06-20T16:52:52.089448Z","shell.execute_reply.started":"2023-06-20T16:52:52.068013Z","shell.execute_reply":"2023-06-20T16:52:52.088334Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                   text  \\\n0     Strengere einheitliche Regeln hinzubekommen, u...   \n1     Alles, was gesagt wurde, sind nur Mittel, und ...   \n2     Nr. 177\\nMan hört, Sie erkundigten sich nach I...   \n3     Mehr Leute sollten sich trauen, sich öffentlic...   \n4     Wie kann es sein, dass der Staat dem #Mittelst...   \n...                                                 ...   \n5541  Das Verbrennen von #Holz und Pellets ist nicht...   \n5542  #Merz zu Ende gedacht: Damit die jüngere Gener...   \n5543  Kommt ihr mit auf einen Spaziergang im Schnee ...   \n5544  Eben fuhr ein ca. 3 jähriger in einem Mercedes...   \n5545  \"Wer eine solche App hat, sollte auch zuerst w...   \n\n                                                    url  \\\n0     https://twitter.com/2857610675/status/13360661...   \n1     https://twitter.com/1307748781945954306/status...   \n2     https://twitter.com/1185108027994714112/status...   \n3     https://twitter.com/1134270667661217794/status...   \n4     https://twitter.com/910913583168573440/status/...   \n...                                                 ...   \n5541  https://twitter.com/robin_wood/status/12598249...   \n5542  https://twitter.com/UlrichSchneider/status/123...   \n5543  https://twitter.com/HHguidemelanie/status/1355...   \n5544  https://twitter.com/demutsch/status/1256161453...   \n5545  https://twitter.com/MissBJArmstrong/status/125...   \n\n        manifestolabel_human  \n0                  welfare +  \n1     traditional morality +  \n2      political authority +  \n3           social justice +  \n4        market regulation +  \n...                      ...  \n5541      environmentalism +  \n5542               welfare +  \n5543               undefined  \n5544               undefined  \n5545  freedom/human rights +  \n\n[5546 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>url</th>\n      <th>manifestolabel_human</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Strengere einheitliche Regeln hinzubekommen, u...</td>\n      <td>https://twitter.com/2857610675/status/13360661...</td>\n      <td>welfare +</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Alles, was gesagt wurde, sind nur Mittel, und ...</td>\n      <td>https://twitter.com/1307748781945954306/status...</td>\n      <td>traditional morality +</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Nr. 177\\nMan hört, Sie erkundigten sich nach I...</td>\n      <td>https://twitter.com/1185108027994714112/status...</td>\n      <td>political authority +</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Mehr Leute sollten sich trauen, sich öffentlic...</td>\n      <td>https://twitter.com/1134270667661217794/status...</td>\n      <td>social justice +</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Wie kann es sein, dass der Staat dem #Mittelst...</td>\n      <td>https://twitter.com/910913583168573440/status/...</td>\n      <td>market regulation +</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5541</th>\n      <td>Das Verbrennen von #Holz und Pellets ist nicht...</td>\n      <td>https://twitter.com/robin_wood/status/12598249...</td>\n      <td>environmentalism +</td>\n    </tr>\n    <tr>\n      <th>5542</th>\n      <td>#Merz zu Ende gedacht: Damit die jüngere Gener...</td>\n      <td>https://twitter.com/UlrichSchneider/status/123...</td>\n      <td>welfare +</td>\n    </tr>\n    <tr>\n      <th>5543</th>\n      <td>Kommt ihr mit auf einen Spaziergang im Schnee ...</td>\n      <td>https://twitter.com/HHguidemelanie/status/1355...</td>\n      <td>undefined</td>\n    </tr>\n    <tr>\n      <th>5544</th>\n      <td>Eben fuhr ein ca. 3 jähriger in einem Mercedes...</td>\n      <td>https://twitter.com/demutsch/status/1256161453...</td>\n      <td>undefined</td>\n    </tr>\n    <tr>\n      <th>5545</th>\n      <td>\"Wer eine solche App hat, sollte auch zuerst w...</td>\n      <td>https://twitter.com/MissBJArmstrong/status/125...</td>\n      <td>freedom/human rights +</td>\n    </tr>\n  </tbody>\n</table>\n<p>5546 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import TFXLMRobertaForSequenceClassification, XLMRobertaTokenizer\n\n# Load data\n\n\n# Encode labels\nle = LabelEncoder()\ndf['labels'] = le.fit_transform(df['manifestolabel_human'])\n","metadata":{"execution":{"iopub.status.busy":"2023-06-20T16:52:52.091084Z","iopub.execute_input":"2023-06-20T16:52:52.091472Z","iopub.status.idle":"2023-06-20T16:52:52.101472Z","shell.execute_reply.started":"2023-06-20T16:52:52.091436Z","shell.execute_reply":"2023-06-20T16:52:52.100443Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import TFXLMRobertaForSequenceClassification, XLMRobertaTokenizer\n\n\n\n\n\n\n# Tokenize tweets\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\ninputs = tokenizer(df['text'].to_list(), max_length=512, truncation=True, padding=True, return_tensors='tf')\n\n# Extract inputs and labels\ninput_ids = inputs['input_ids']\nattention_mask = inputs['attention_mask']\nlabels = df['labels'].to_numpy()\n\n# Split data into train and test sets\ntrain_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids.numpy(), labels, test_size=0.2, random_state=42)\n\n# Convert them into TensorFlow tensors\ntrain_inputs = tf.convert_to_tensor(train_inputs)\nval_inputs = tf.convert_to_tensor(val_inputs)\ntrain_labels = tf.convert_to_tensor(train_labels)\nval_labels = tf.convert_to_tensor(val_labels)\n\n# Prepare TensorFlow dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': train_inputs}, train_labels)).batch(16)\nval_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_inputs}, val_labels)).batch(16)\n\n# Load model\nmodel = TFXLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=len(le.classes_))\n\n# Define optimizer and loss function\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Compile model\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\n# Train model\nmodel.fit(train_dataset, epochs=10, validation_data=val_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-20T16:52:52.103110Z","iopub.execute_input":"2023-06-20T16:52:52.103968Z","iopub.status.idle":"2023-06-20T17:25:09.918393Z","shell.execute_reply.started":"2023-06-20T16:52:52.103934Z","shell.execute_reply":"2023-06-20T17:25:09.917359Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f270e5db765a413f96ab76a69c3eb2fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7742b19456ae46d1a7ddb22dd2bf4823"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73cb7dec9d7c44a28734bf78976fec81"}},"metadata":{}},{"name":"stderr","text":"All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n278/278 [==============================] - 237s 661ms/step - loss: 3.1941 - accuracy: 0.2090 - val_loss: 3.0574 - val_accuracy: 0.2288\nEpoch 2/10\n278/278 [==============================] - 181s 650ms/step - loss: 3.0839 - accuracy: 0.2182 - val_loss: 3.0587 - val_accuracy: 0.2288\nEpoch 3/10\n278/278 [==============================] - 181s 652ms/step - loss: 3.0827 - accuracy: 0.2182 - val_loss: 3.0675 - val_accuracy: 0.2288\nEpoch 4/10\n278/278 [==============================] - 181s 653ms/step - loss: 3.0816 - accuracy: 0.2182 - val_loss: 3.0587 - val_accuracy: 0.2288\nEpoch 5/10\n278/278 [==============================] - 181s 651ms/step - loss: 3.0847 - accuracy: 0.2182 - val_loss: 3.0590 - val_accuracy: 0.2288\nEpoch 6/10\n278/278 [==============================] - 181s 652ms/step - loss: 3.0850 - accuracy: 0.2182 - val_loss: 3.0645 - val_accuracy: 0.2288\nEpoch 7/10\n278/278 [==============================] - 181s 653ms/step - loss: 3.0789 - accuracy: 0.2182 - val_loss: 3.0579 - val_accuracy: 0.2288\nEpoch 8/10\n278/278 [==============================] - 189s 679ms/step - loss: 3.0776 - accuracy: 0.2182 - val_loss: 3.0559 - val_accuracy: 0.2288\nEpoch 9/10\n278/278 [==============================] - 181s 652ms/step - loss: 3.0805 - accuracy: 0.2180 - val_loss: 3.0576 - val_accuracy: 0.2288\nEpoch 10/10\n278/278 [==============================] - 181s 653ms/step - loss: 3.0805 - accuracy: 0.2182 - val_loss: 3.0555 - val_accuracy: 0.2288\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7e95bd5eaa40>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TFXLMRobertaForSequenceClassification, XLMRobertaTokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nimport tensorflow as tf\n\n# Assuming df and le.classes_ exist in your workspace\n\n# Tokenize tweets\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\ninputs = tokenizer(df['text'].to_list(), max_length=512, truncation=True, padding=True, return_tensors='tf')\n\n# Extract inputs and labels\ninput_ids = inputs['input_ids']\nattention_mask = inputs['attention_mask']\nlabels = df['labels'].to_numpy()\n\n# Split data into train and test sets\ntrain_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids.numpy(), labels, test_size=0.2, random_state=42)\n\n# Convert them into TensorFlow tensors\ntrain_inputs = tf.convert_to_tensor(train_inputs)\nval_inputs = tf.convert_to_tensor(val_inputs)\ntrain_labels = tf.convert_to_tensor(train_labels)\nval_labels = tf.convert_to_tensor(val_labels)\n\n# Prepare TensorFlow dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': train_inputs}, train_labels)).batch(16)\nval_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_inputs}, val_labels)).batch(16)\n\n# Load model\nmodel = TFXLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=len(le.classes_))\n\n# Define optimizer and loss function\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Compile model\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\n# Define Early Stopping and Learning Rate Reduction on Plateau callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3)\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n\n# Train model\nmodel.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[early_stopping, lr_scheduler])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}